# Image Retrieval

[![Open Source? Yes!](https://badgen.net/badge/Open%20Source%20%3F/Yes%21/blue?icon=github)](https://github.com/Naereen/badges/) [![made-with-python](https://img.shields.io/badge/Made%20with-Python-1f425f.svg)](https://www.python.org/) [![GitHub](https://badgen.net/badge/icon/github?icon=github&label)](https://github.com)

![framework](D:\gitrepo\image-retrieval\img\framework.svg)

<h4 align="center">
    <p>
        <a href="https://github.com/Aldenhovel/image-retrieval/blob/main/readme.md">中文</a> |
        <b>English</b> 
    <p>
</h4>

## Introduction

This is a very stitched image-text retrieval framework. The core is to generate a description of the image content and then use the similarity of the description text to calculate qualified pictures. The advantages are:

- Ready to use, each network can be used together without training.
- Custom text can be manually added to the image description so that image retrieval can consider information that is not recognized in the image.
- Combines image subtitles, target detection, text classification and text embedding models, and the effect is okay.



## Module

### Image Captioning Model

The image subtitle model is responsible for generating text descriptions of image content and plays the most important role in the framework. The [noamrot/FuseCap_Image_Captioning](https://hf-mirror.com/noamrot/FuseCap_Image_Captioning) model is used here, learn more:

>
>
>Rotstein, Noam, et al. "FuseCap: Leveraging Large Language Models to Fuse Visual Data into Enriched Image Captions." *arXiv preprint arXiv:2305.17718* (2023).

### Object Detection Model

The object detection model is responsible for detecting the target objects existing in the image and supplementing the generated image subtitles in the framework. It mainly emphasizes the quantitative relationship of the objects and prevents the image subtitles from missing elements or getting the wrong number. The [ultralytics/YOLOv8](https://github.com/ultralytics/ultralytics) model is used here, learn more:

>
>
>Jocher, G., Chaurasia, A., & Qiu, J. (2023). Ultralytics YOLO (Version 8.0.0) [Computer software]. https://github.com/ultralytics/ultralytics

### Text Classification Model

The text classification model is responsible for classifying the comprehensive description content of the image generated by image subtitles and target detection. The generated text categories will be used as guidance clues in subsequent text embedding to improve the accuracy of text embedding. The [cardiffnlp/tweet-topic-21-multi](https://hf-mirror.com/cardiffnlp/tweet-topic-21-multi) model is used here, which can distinguish text into 21 categories. Learn more :

>
>
>Antypas, Dimosthenis, et al. "Twitter topic classification." *arXiv preprint arXiv:2209.09824* (2022).

### Text Embedding Model

The text embedding model is responsible for converting text into feature vectors and is the most important link in calculating text similarity. The [hkunlp/instructor-large](https://hf-mirror.com/hkunlp/instructor-large) model is used here. Based on the two parameters of instruction and sentence, in this framework, the image is comprehensively described as sentence, and the text is The category is encoded as introduction and calculated to obtain the feature vector. learn more:

>
>
>Su, Hongjin, et al. "One embedder, any task: Instruction-finetuned text embeddings." *arXiv preprint arXiv:2212.09741* (2022).



## Requirements

```
torch==1.13.1
numpy==1.21.6
matplotlib==3.5.3
transformers==4.31.0
scipy==1.7.3
ultralytics==8.0.228
tqdm==4.66.1
```

Due to the file size limit, you also need to manually move the HuggingFace model `bin` file download to the two corresponding directories `cardiffnlp/tweet-topic-21-multi/` and `noamrot/FuseCap/`:

- [tweet-topic-21-multi](https://hf-mirror.com/cardiffnlp/tweet-topic-21-multi/blob/main/pytorch_model.bin)
- [fusecap](https://hf-mirror.com/noamrot/FuseCap_Image_Captioning/blob/main/pytorch_model.bin)



## Get Start

**Check module availability** Use `test_utils.ipynb` to check whether each model has been deployed correctly.

**Create image feature index** Use `build_index.ipynb` to build a feature index for the `.jpg` image file in `gallery/`, and the result will be saved to `tmp/`.

**Image retrieval** Use `retrieval_image.ipynb` to retrieve images (image feature index needs to be established first).



## Examples

```
a plane flying in the sky .
```

![exam0](D:\gitrepo\image-retrieval\img\example0.png)

```
a bus in the street .
```

![exam1](D:\gitrepo\image-retrieval\img\example1.png)



## Experimental Data

- Image retrieval experiment on Filckr8k

  |                               |  @1   |  @5   |  @20  | avg/1k |
  | :---------------------------: | :---: | :---: | :---: | :----: |
  |      YOLOv8 + Instructor      | 14.9% | 33.6% | 52.4% | 66.95  |
  |     FuseCap + Instructor      | 39.7% | 67.3% | 87.3% | 12.53  |
  | FuseCap + YOLOv8 + Instructor | 43.2% | 69.6% | 89.2% | 10.76  |

- Image retrieval experiment on Flickr30k

  |                               |  @1   |  @5   |  @20  | avg/1k |
  | :---------------------------: | :---: | :---: | :---: | :----: |
  |      YOLOv8 + Instructor      | 15.4% | 31.6% | 50.7% | 85.34  |
  |     FuseCap + Instructor      | 42.8% | 70.2% | 86.3% | 16.27  |
  | FuseCap + YOLOv8 + Instructor | 47.6% | 74.0% | 89.0% | 11.54  |

- Image retrieval experiment on MSCOCO

  |                               |  @1   |  @5   |  @20  | avg/1k |
  | :---------------------------: | :---: | :---: | :---: | :----: |
  |      YOLOv8 + Instructor      | 12.7% | 25.4% | 45.8% | 120.35 |
  |     FuseCap + Instructor      | 34.1% | 59.4% | 80.4% | 15.03  |
  | FuseCap + YOLOv8 + Instructor | 37.9% | 63.7% | 83.7% | 12.88  |



## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=aldenhovel/image-retrieval&type=Date)](https://star-history.com/#aldenhovel/image-retrieval&Date)

